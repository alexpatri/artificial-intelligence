{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexpatri/artificial-intelligence/blob/main/1-2-tratamento-de-texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owvoDbN6Vfdp"
      },
      "source": [
        "**O que √© processamento de linguagem natural?**\n",
        "\n",
        "O processamento de linguagem natural √© um enorme campo de estudo e pr√°tica usada ativamente que visa dar sentido √† linguagem usando estat√≠sticas e computadores. Nas pr√≥ximas atividades voc√™ aprender√° alguns dos fundamentos da PLN, que o ajudar√£o a passar de t√≥picos simples para t√≥picos mais dif√≠ceis e avan√ßados. Voc√™ ter√° alguma exposi√ß√£o aos desafios da √°rea, como identifica√ß√£o de t√≥picos e classifica√ß√£o de textos. Algumas √°reas interessantes da PLN que voc√™ deve ter ouvido falar s√£o: identifica√ß√£o de t√≥picos, chatbots, classifica√ß√£o de texto, tradu√ß√£o, an√°lise de sentimento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egPbQkeGVyyp"
      },
      "source": [
        "Express√µes regulares s√£o strings que voc√™ pode usar e que possuem uma sintaxe especial, que permite combinar padr√µes e encontrar outras strings. Um padr√£o √© uma s√©rie de letras ou s√≠mbolos que podem ser mapeados para um texto real ou palavras ou pontua√ß√£o. Voc√™ pode usar express√µes regulares para fazer coisas como encontrar links em uma p√°gina da web, analisar endere√ßos de e-mail e remover strings ou caracteres indesejados. Express√µes regulares s√£o freq√ºentemente chamadas de regex e podem ser usadas facilmente com python por meio da biblioteca `re`. Aqui temos uma importa√ß√£o simples da biblioteca. Podemos combinar uma substring usando o m√©todo re.match que combina um padr√£o com uma string. Ele pega o padr√£o como o primeiro argumento, a string como o segundo e retorna um objeto de correspond√™ncia. Tamb√©m podemos usar padr√µes especiais que o regex entende, como o \\ w + que corresponder√° a uma palavra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAnErySrWBEB"
      },
      "source": [
        "Existem centenas de caracteres e padr√µes que voc√™ pode aprender e memorizar com express√µes regulares, mas para come√ßar, compartilhamos alguns padr√µes comuns. O primeiro padr√£o \\ w que j√° vimos, √© usado para combinar palavras. O padr√£o \\ d nos permite combinar d√≠gitos, o que pode ser √∫til quando voc√™ precisa encontr√°-los e separ√°-los em uma string. O padr√£o \\ s corresponde a espa√ßos, o ponto final √© um caractere curinga. O curinga corresponder√° a QUALQUER letra ou s√≠mbolo. Os caracteres + e * permitem que as coisas se tornem gananciosas, agarrando repeti√ß√µes de letras √∫nicas ou padr√µes inteiros. Por exemplo, para corresponder a uma palavra inteira em vez de um caractere, precisamos adicionar o s√≠mbolo + ap√≥s o \\ w. Usar essas classes de caracteres como letras mai√∫sculas as nega, ent√£o o \\ S corresponde a qualquer coisa que n√£o seja um espa√ßo. Voc√™ tamb√©m pode criar um grupo de caracteres que deseja colocando-os entre colchetes, como nosso grupo de letras min√∫sculas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tekFqNS13aoh",
        "outputId": "e1b94f2f-081d-479f-e578-f38179784265"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-T1sJYz7rDL",
        "outputId": "200c2aee-f23f-4c55-ff97-86b46f2cd90a"
      },
      "source": [
        "#https://cheatography.com/davechild/cheat-sheets/regular-expressions/\n",
        "\n",
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]\\w*\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits, my_string))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'I', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbypwpKd9qaH",
        "outputId": "65e5d594-487a-4aa3-f5a9-1e1d10332767"
      },
      "source": [
        "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
        "\n",
        "# Import necessary modules\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[10])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'servant', 'No', 'you', '?', 'together', 'forty-three', 'sovereign', \"'m\", 'maybe', 'That', 'ask', 'Will', 'if', 'winter', 'may', 'plover', 'defeator', 'our', 'five', 'Well', 'on', 'just', 'by', 'needs', 'its', 'pound', 'European', 'a', 'Oh', 'two', 'or', 'seek', 'use', 'strand', '!', 'warmer', 'interested', 'back', 'African', 'using', 'be', 'trusty', 'found', 'does', 'get', 'non-migratory', 'second', 'yeah', 'dorsal', 'Wait', 'I', 'go', 'agree', 'Ridden', 'with', 'wants', 'your', 'yet', 'is', 'Yes', \"'re\", 'martin', 'Halt', 'they', 'It', 'Please', 'England', 'them', 'land', 'Saxons', 'bring', 'all', 'could', 'weight', 'point', 'search', 'grips', 'Mercea', 'zone', 'King', 'Pendragon', 'question', \"'ve\", 'not', 'tropical', 'course', 'migrate', 'one', 'through', 'court', 'coconuts', 'The', 'Where', 'that', 'horse', 'empty', 'castle', 'an', 'beat', 'under', 'house', 'in', 'swallows', 'other', 'where', 'ARTHUR', 'Listen', 'speak', '[', \"'s\", '#', 'kingdom', 'Not', 'are', 'to', 'Arthur', 'strangers', 'carry', 'In', '2', 'suggesting', 'breadth', 'goes', 'every', '1', 'Pull', 'Camelot', \"'em\", 'bird', 'grip', ':', 'of', 'ratios', 'creeper', 'why', 'We', 'matter', 'line', '.', 'So', 'son', 'KING', 'since', 'but', 'fly', 'climes', 'my', 'ounce', 'maintain', 'from', 'coconut', 'Britons', 'south', 'sun', 'wings', 'A', 'there', 'it', 'order', 'minute', 'times', 'guiding', 'simple', 'air-speed', 'Uther', '...', 'What', 'carrying', 'these', 'swallow', 'knights', 'who', \"n't\", 'Am', 'halves', 'You', 'clop', 'ridden', 'master', 'Court', 'mean', 'Found', 'Patsy', 'lord', 'velocity', 'SOLDIER', 'SCENE', 'this', 'husk', 'me', 'Are', 'feathers', 'wind', 'join', 'got', 'Who', 'must', 'anyway', 'carried', 'will', 'right', 'then', 'am', '--', \"'\", 'tell', 'he', 'Supposing', ']', 'the', 'bangin', 'held', 'They', 'here', 'But', ',', 'covered', \"'d\", 'Whoa', 'do', 'length', 'at', 'temperate', 'have', 'snows', 'and'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAdole9fAFsv",
        "outputId": "8f3293f4-fcb0-4fb7-d9c8-329776506d79"
      },
      "source": [
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(), match.end())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "580 588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX_GxIx9D7Z2"
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o2cOfR9EMB_",
        "outputId": "05cefc0f-80ea-472a-91cd-c94c2aa0f340"
      },
      "source": [
        "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
        " '#NLP is super fun! <3 #learning',\n",
        " 'Thanks @datacamp :) #nlp #python']\n",
        "\n",
        "pattern1 = r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "print(hashtags)\n",
        "\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)\n",
        "\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#nlp', '#python']\n",
            "['@datacamp', '#nlp', '#python']\n",
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQKVf8vUEnz7",
        "outputId": "a922a992-8af3-41c8-98ab-ef3603df7b77"
      },
      "source": [
        "german_text = 'Wann gehen wir Pizza essen? üçï Und f√§hrst du mit √úber? üöï'\n",
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)\n",
        "\n",
        "# Tokenize and print only capital words\n",
        "capital_words = r\"[A-Z√ú]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words))\n",
        "\n",
        "# Tokenize and print only emoji\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(german_text, emoji))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', 'üçï', 'Und', 'f√§hrst', 'du', 'mit', '√úber', '?', 'üöï']\n",
            "['Wann', 'Pizza', 'Und', '√úber']\n",
            "['üçï', 'üöï']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "NiXe2JmGFiev",
        "outputId": "3ca8f402-1c92-467a-82e3-e0761d04e22a"
      },
      "source": [
        "#holy_grail = \"\"\n",
        "holy_grail = open('holy_grail.txt', 'r')\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split the script into lines: lines\n",
        "lines = holy_grail.readlines()\n",
        "\n",
        "# Replace all script lines for speaker\n",
        "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
        "lines = [re.sub(pattern, '', l) for l in lines]\n",
        "\n",
        "# Tokenize each line: tokenized_lines\n",
        "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
        "\n",
        "# Make a frequency list of lengths: line_num_words\n",
        "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "# Plot a histogram of the line lengths\n",
        "plt.hist(line_num_words)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "Counter(word_tokenize(\"Inserir Inserir qualquer texto nesse espa√ßo e verificar o rsultado do texto\"))"
      ],
      "metadata": {
        "id": "Vnt0KYeBzYNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b610e9d6-2191-49e7-bd27-e1af19df8a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Inserir': 2,\n",
              "         'qualquer': 1,\n",
              "         'texto': 2,\n",
              "         'nesse': 1,\n",
              "         'espa√ßo': 1,\n",
              "         'e': 1,\n",
              "         'verificar': 1,\n",
              "         'o': 1,\n",
              "         'rsultado': 1,\n",
              "         'do': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKEl_gKXXpnE"
      },
      "source": [
        "Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QZnxbCsXni8",
        "outputId": "d6f87005-675a-4a99-d242-3af08d7b57f3"
      },
      "source": [
        "article = open('debugging.txt', 'r')\n",
        "\n",
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article.read())\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 151), ('the', 150), ('.', 89), ('of', 81), ('to', 63), ('a', 60), ('in', 44), (\"''\", 42), ('and', 41), ('(', 40)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMJh6fMhYxzh",
        "outputId": "03bc72aa-223e-4cda-9ec1-21af772ad201"
      },
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('debugging', 36), ('system', 25), ('software', 16), ('bug', 15), ('problem', 15), ('computer', 14), ('tool', 14), ('process', 13), ('term', 13), ('used', 12), ('http', 11), ('program', 11), ('debugger', 10), ('programming', 9), ('technique', 9), ('language', 9), ('code', 8), ('example', 8), ('check', 8), ('error', 7), ('also', 7), ('make', 7), ('programmer', 6), ('may', 6), ('acm', 6), ('would', 6), ('user', 6), ('case', 6), ('test', 6), ('ref', 6), ('see', 5), ('memory', 5), ('change', 5), ('debug', 5), ('article', 5), ('hardware', 5), ('task', 5), ('execution', 5), ('source', 5), ('wolf', 5), ('cite', 5), ('control', 4), ('testing', 4), ('dump', 4), ('design', 4), ('early', 4), ('proceeding', 4), ('use', 4), ('computing', 4), ('national', 4), ('anomaly', 4), ('impact', 4), ('word', 4), ('might', 4), ('determine', 4), ('often', 4), ('value', 4), ('variable', 4), ('original', 4), ('state', 4), ('tracing', 4), ('different', 4), ('fence', 4), ('algorithm', 4), ('embedded', 4), ('file', 3), ('mark', 3), ('moth', 3), ('grace', 3), ('hopper', 3), ('journal', 3), ('first', 3), ('meeting', 3), ('common', 3), ('various', 3), ('avoid', 3), ('made', 3), ('issue', 3), ('developer', 3), ('simple', 3), ('analysis', 3), ('breakpoints', 3), ('easier', 3), ('exception', 3), ('specific', 3), ('useful', 3), ('within', 3), ('detected', 3), ('reproduce', 3), ('environment', 3), ('crash', 3), ('line', 3), ('using', 3), ('part', 3), ('print', 3), ('statement', 3), ('command', 3), ('tron', 3), ('version', 3), ('remote', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('portuguese')"
      ],
      "metadata": {
        "id": "qL-VavBv3iP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df6ec7e-3b53-4ec4-8d48-2397bb6ccd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " '√†',\n",
              " 'ao',\n",
              " 'aos',\n",
              " 'aquela',\n",
              " 'aquelas',\n",
              " 'aquele',\n",
              " 'aqueles',\n",
              " 'aquilo',\n",
              " 'as',\n",
              " '√†s',\n",
              " 'at√©',\n",
              " 'com',\n",
              " 'como',\n",
              " 'da',\n",
              " 'das',\n",
              " 'de',\n",
              " 'dela',\n",
              " 'delas',\n",
              " 'dele',\n",
              " 'deles',\n",
              " 'depois',\n",
              " 'do',\n",
              " 'dos',\n",
              " 'e',\n",
              " '√©',\n",
              " 'ela',\n",
              " 'elas',\n",
              " 'ele',\n",
              " 'eles',\n",
              " 'em',\n",
              " 'entre',\n",
              " 'era',\n",
              " 'eram',\n",
              " '√©ramos',\n",
              " 'essa',\n",
              " 'essas',\n",
              " 'esse',\n",
              " 'esses',\n",
              " 'esta',\n",
              " 'est√°',\n",
              " 'estamos',\n",
              " 'est√£o',\n",
              " 'estar',\n",
              " 'estas',\n",
              " 'estava',\n",
              " 'estavam',\n",
              " 'est√°vamos',\n",
              " 'este',\n",
              " 'esteja',\n",
              " 'estejam',\n",
              " 'estejamos',\n",
              " 'estes',\n",
              " 'esteve',\n",
              " 'estive',\n",
              " 'estivemos',\n",
              " 'estiver',\n",
              " 'estivera',\n",
              " 'estiveram',\n",
              " 'estiv√©ramos',\n",
              " 'estiverem',\n",
              " 'estivermos',\n",
              " 'estivesse',\n",
              " 'estivessem',\n",
              " 'estiv√©ssemos',\n",
              " 'estou',\n",
              " 'eu',\n",
              " 'foi',\n",
              " 'fomos',\n",
              " 'for',\n",
              " 'fora',\n",
              " 'foram',\n",
              " 'f√¥ramos',\n",
              " 'forem',\n",
              " 'formos',\n",
              " 'fosse',\n",
              " 'fossem',\n",
              " 'f√¥ssemos',\n",
              " 'fui',\n",
              " 'h√°',\n",
              " 'haja',\n",
              " 'hajam',\n",
              " 'hajamos',\n",
              " 'h√£o',\n",
              " 'havemos',\n",
              " 'haver',\n",
              " 'hei',\n",
              " 'houve',\n",
              " 'houvemos',\n",
              " 'houver',\n",
              " 'houvera',\n",
              " 'houver√°',\n",
              " 'houveram',\n",
              " 'houv√©ramos',\n",
              " 'houver√£o',\n",
              " 'houverei',\n",
              " 'houverem',\n",
              " 'houveremos',\n",
              " 'houveria',\n",
              " 'houveriam',\n",
              " 'houver√≠amos',\n",
              " 'houvermos',\n",
              " 'houvesse',\n",
              " 'houvessem',\n",
              " 'houv√©ssemos',\n",
              " 'isso',\n",
              " 'isto',\n",
              " 'j√°',\n",
              " 'lhe',\n",
              " 'lhes',\n",
              " 'mais',\n",
              " 'mas',\n",
              " 'me',\n",
              " 'mesmo',\n",
              " 'meu',\n",
              " 'meus',\n",
              " 'minha',\n",
              " 'minhas',\n",
              " 'muito',\n",
              " 'na',\n",
              " 'n√£o',\n",
              " 'nas',\n",
              " 'nem',\n",
              " 'no',\n",
              " 'nos',\n",
              " 'n√≥s',\n",
              " 'nossa',\n",
              " 'nossas',\n",
              " 'nosso',\n",
              " 'nossos',\n",
              " 'num',\n",
              " 'numa',\n",
              " 'o',\n",
              " 'os',\n",
              " 'ou',\n",
              " 'para',\n",
              " 'pela',\n",
              " 'pelas',\n",
              " 'pelo',\n",
              " 'pelos',\n",
              " 'por',\n",
              " 'qual',\n",
              " 'quando',\n",
              " 'que',\n",
              " 'quem',\n",
              " 's√£o',\n",
              " 'se',\n",
              " 'seja',\n",
              " 'sejam',\n",
              " 'sejamos',\n",
              " 'sem',\n",
              " 'ser',\n",
              " 'ser√°',\n",
              " 'ser√£o',\n",
              " 'serei',\n",
              " 'seremos',\n",
              " 'seria',\n",
              " 'seriam',\n",
              " 'ser√≠amos',\n",
              " 'seu',\n",
              " 'seus',\n",
              " 's√≥',\n",
              " 'somos',\n",
              " 'sou',\n",
              " 'sua',\n",
              " 'suas',\n",
              " 'tamb√©m',\n",
              " 'te',\n",
              " 'tem',\n",
              " 't√©m',\n",
              " 'temos',\n",
              " 'tenha',\n",
              " 'tenham',\n",
              " 'tenhamos',\n",
              " 'tenho',\n",
              " 'ter√°',\n",
              " 'ter√£o',\n",
              " 'terei',\n",
              " 'teremos',\n",
              " 'teria',\n",
              " 'teriam',\n",
              " 'ter√≠amos',\n",
              " 'teu',\n",
              " 'teus',\n",
              " 'teve',\n",
              " 'tinha',\n",
              " 'tinham',\n",
              " 't√≠nhamos',\n",
              " 'tive',\n",
              " 'tivemos',\n",
              " 'tiver',\n",
              " 'tivera',\n",
              " 'tiveram',\n",
              " 'tiv√©ramos',\n",
              " 'tiverem',\n",
              " 'tivermos',\n",
              " 'tivesse',\n",
              " 'tivessem',\n",
              " 'tiv√©ssemos',\n",
              " 'tu',\n",
              " 'tua',\n",
              " 'tuas',\n",
              " 'um',\n",
              " 'uma',\n",
              " 'voc√™',\n",
              " 'voc√™s',\n",
              " 'vos']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles = open('articles.txt', 'r')\n",
        "articles = articles.readlines()\n",
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_articles = [word_tokenize(doc.lower()) for doc in articles]\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(tokenized_articles)\n",
        "\n",
        "print(dictionary.token2id.get(\"transport\"))\n",
        "\n",
        "#dictionary.token2id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS6wUfsuW8DY",
        "outputId": "a17586ce-20fa-4c49-fd05-9f65a274eb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "my_documents = ['The movie was about a spaceship and aliens.',\n",
        "                'I really liked the movie!',\n",
        "                'Awesome action scenes, but boring characters.',\n",
        "                'The movie was awful! I hate alien films.',\n",
        "                'Space is cool! I liked the movie.',\n",
        "                'More space films, please!',]\n",
        "\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "dictionary.token2id"
      ],
      "metadata": {
        "id": "MyYvrVH2a_Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23f8508-40f8-4c24-ed5c-e893efcff5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 0,\n",
              " 'a': 1,\n",
              " 'about': 2,\n",
              " 'aliens': 3,\n",
              " 'and': 4,\n",
              " 'movie': 5,\n",
              " 'spaceship': 6,\n",
              " 'the': 7,\n",
              " 'was': 8,\n",
              " '!': 9,\n",
              " 'i': 10,\n",
              " 'liked': 11,\n",
              " 'really': 12,\n",
              " ',': 13,\n",
              " 'action': 14,\n",
              " 'awesome': 15,\n",
              " 'boring': 16,\n",
              " 'but': 17,\n",
              " 'characters': 18,\n",
              " 'scenes': 19,\n",
              " 'alien': 20,\n",
              " 'awful': 21,\n",
              " 'films': 22,\n",
              " 'hate': 23,\n",
              " 'cool': 24,\n",
              " 'is': 25,\n",
              " 'space': 26,\n",
              " 'more': 27,\n",
              " 'please': 28}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "corpus"
      ],
      "metadata": {
        "id": "uidMzc8TrMtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da293b2-f836-4d83-f59b-31b3ea492f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
              " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
              " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
              " [(0, 1),\n",
              "  (5, 1),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 1),\n",
              "  (10, 1),\n",
              "  (20, 1),\n",
              "  (21, 1),\n",
              "  (22, 1),\n",
              "  (23, 1)],\n",
              " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
              " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "tfidf = TfidfModel(corpus)\n",
        "tfidf[corpus[1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7N_TYwmrf1K",
        "outputId": "74f9899a-b300-4fc2-963b-ef341baffc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(5, 0.1746298276735174),\n",
              " (7, 0.1746298276735174),\n",
              " (9, 0.1746298276735174),\n",
              " (10, 0.29853166221463673),\n",
              " (11, 0.47316148988815415),\n",
              " (12, 0.7716931521027908)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFR40dSsbQ6J"
      },
      "source": [
        "Gensim - https://radimrehurek.com/gensim/"
      ]
    }
  ]
}