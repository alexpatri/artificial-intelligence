{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexpatri/artificial-intelligence/blob/main/1-2-tratamento-de-texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owvoDbN6Vfdp"
      },
      "source": [
        "**O que é processamento de linguagem natural?**\n",
        "\n",
        "O processamento de linguagem natural é um enorme campo de estudo e prática usada ativamente que visa dar sentido à linguagem usando estatísticas e computadores. Nas próximas atividades você aprenderá alguns dos fundamentos da PLN, que o ajudarão a passar de tópicos simples para tópicos mais difíceis e avançados. Você terá alguma exposição aos desafios da área, como identificação de tópicos e classificação de textos. Algumas áreas interessantes da PLN que você deve ter ouvido falar são: identificação de tópicos, chatbots, classificação de texto, tradução, análise de sentimento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egPbQkeGVyyp"
      },
      "source": [
        "Expressões regulares são strings que você pode usar e que possuem uma sintaxe especial, que permite combinar padrões e encontrar outras strings. Um padrão é uma série de letras ou símbolos que podem ser mapeados para um texto real ou palavras ou pontuação. Você pode usar expressões regulares para fazer coisas como encontrar links em uma página da web, analisar endereços de e-mail e remover strings ou caracteres indesejados. Expressões regulares são freqüentemente chamadas de regex e podem ser usadas facilmente com python por meio da biblioteca `re`. Aqui temos uma importação simples da biblioteca. Podemos combinar uma substring usando o método re.match que combina um padrão com uma string. Ele pega o padrão como o primeiro argumento, a string como o segundo e retorna um objeto de correspondência. Também podemos usar padrões especiais que o regex entende, como o \\ w + que corresponderá a uma palavra."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAnErySrWBEB"
      },
      "source": [
        "Existem centenas de caracteres e padrões que você pode aprender e memorizar com expressões regulares, mas para começar, compartilhamos alguns padrões comuns. O primeiro padrão \\ w que já vimos, é usado para combinar palavras. O padrão \\ d nos permite combinar dígitos, o que pode ser útil quando você precisa encontrá-los e separá-los em uma string. O padrão \\ s corresponde a espaços, o ponto final é um caractere curinga. O curinga corresponderá a QUALQUER letra ou símbolo. Os caracteres + e * permitem que as coisas se tornem gananciosas, agarrando repetições de letras únicas ou padrões inteiros. Por exemplo, para corresponder a uma palavra inteira em vez de um caractere, precisamos adicionar o símbolo + após o \\ w. Usar essas classes de caracteres como letras maiúsculas as nega, então o \\ S corresponde a qualquer coisa que não seja um espaço. Você também pode criar um grupo de caracteres que deseja colocando-os entre colchetes, como nosso grupo de letras minúsculas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tekFqNS13aoh",
        "outputId": "e1b94f2f-081d-479f-e578-f38179784265"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('stopwords')\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-T1sJYz7rDL",
        "outputId": "200c2aee-f23f-4c55-ff97-86b46f2cd90a"
      },
      "source": [
        "#https://cheatography.com/davechild/cheat-sheets/regular-expressions/\n",
        "\n",
        "my_string = \"Let's write RegEx!  Won't that be fun?  I sure think so.  Can you find 4 sentences?  Or perhaps, all 19 words?\"\n",
        "# Write a pattern to match sentence endings: sentence_endings\n",
        "sentence_endings = r\"[.?!]\"\n",
        "\n",
        "# Split my_string on sentence endings and print the result\n",
        "print(re.split(sentence_endings, my_string))\n",
        "\n",
        "# Find all capitalized words in my_string and print the result\n",
        "capitalized_words = r\"[A-Z]\\w*\"\n",
        "print(re.findall(capitalized_words, my_string))\n",
        "\n",
        "# Split my_string on spaces and print the result\n",
        "spaces = r\"\\s+\"\n",
        "print(re.split(spaces, my_string))\n",
        "\n",
        "# Find all digits in my_string and print the result\n",
        "digits = r\"\\d+\"\n",
        "print(re.findall(digits, my_string))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Let's write RegEx\", \"  Won't that be fun\", '  I sure think so', '  Can you find 4 sentences', '  Or perhaps, all 19 words', '']\n",
            "['Let', 'RegEx', 'Won', 'I', 'Can', 'Or']\n",
            "[\"Let's\", 'write', 'RegEx!', \"Won't\", 'that', 'be', 'fun?', 'I', 'sure', 'think', 'so.', 'Can', 'you', 'find', '4', 'sentences?', 'Or', 'perhaps,', 'all', '19', 'words?']\n",
            "['4', '19']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tbypwpKd9qaH",
        "outputId": "65e5d594-487a-4aa3-f5a9-1e1d10332767"
      },
      "source": [
        "scene_one = \"SCENE 1: [wind] [clop clop clop] \\nKING ARTHUR: Whoa there!  [clop clop clop] \\nSOLDIER #1: Halt!  Who goes there?\\nARTHUR: It is I, Arthur, son of Uther Pendragon, from the castle of Camelot.  King of the Britons, defeator of the Saxons, sovereign of all England!\\nSOLDIER #1: Pull the other one!\\nARTHUR: I am, ...  and this is my trusty servant Patsy.  We have ridden the length and breadth of the land in search of knights who will join me in my court at Camelot.  I must speak with your lord and master.\\nSOLDIER #1: What?  Ridden on a horse?\\nARTHUR: Yes!\\nSOLDIER #1: You're using coconuts!\\nARTHUR: What?\\nSOLDIER #1: You've got two empty halves of coconut and you're bangin' 'em together.\\nARTHUR: So?  We have ridden since the snows of winter covered this land, through the kingdom of Mercea, through--\\nSOLDIER #1: Where'd you get the coconuts?\\nARTHUR: We found them.\\nSOLDIER #1: Found them?  In Mercea?  The coconut's tropical!\\nARTHUR: What do you mean?\\nSOLDIER #1: Well, this is a temperate zone.\\nARTHUR: The swallow may fly south with the sun or the house martin or the plover may seek warmer climes in winter, yet these are not strangers to our land?\\nSOLDIER #1: Are you suggesting coconuts migrate?\\nARTHUR: Not at all.  They could be carried.\\nSOLDIER #1: What?  A swallow carrying a coconut?\\nARTHUR: It could grip it by the husk!\\nSOLDIER #1: It's not a question of where he grips it!  It's a simple question of weight ratios!  A five ounce bird could not carry a one pound coconut.\\nARTHUR: Well, it doesn't matter.  Will you go and tell your master that Arthur from the Court of Camelot is here.\\nSOLDIER #1: Listen.  In order to maintain air-speed velocity, a swallow needs to beat its wings forty-three times every second, right?\\nARTHUR: Please!\\nSOLDIER #1: Am I right?\\nARTHUR: I'm not interested!\\nSOLDIER #2: It could be carried by an African swallow!\\nSOLDIER #1: Oh, yeah, an African swallow maybe, but not a European swallow.  That's my point.\\nSOLDIER #2: Oh, yeah, I agree with that.\\nARTHUR: Will you ask your master if he wants to join my court at Camelot?!\\nSOLDIER #1: But then of course a-- African swallows are non-migratory.\\nSOLDIER #2: Oh, yeah...\\nSOLDIER #1: So they couldn't bring a coconut back anyway...  [clop clop clop] \\nSOLDIER #2: Wait a minute!  Supposing two swallows carried it together?\\nSOLDIER #1: No, they'd have to have it on a line.\\nSOLDIER #2: Well, simple!  They'd just use a strand of creeper!\\nSOLDIER #1: What, held under the dorsal guiding feathers?\\nSOLDIER #2: Well, why not?\\n\"\n",
        "\n",
        "# Import necessary modules\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split scene_one into sentences: sentences\n",
        "sentences = sent_tokenize(scene_one)\n",
        "\n",
        "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n",
        "tokenized_sent = word_tokenize(sentences[10])\n",
        "\n",
        "# Make a set of unique tokens in the entire scene: unique_tokens\n",
        "unique_tokens = set(word_tokenize(scene_one))\n",
        "\n",
        "# Print the unique tokens result\n",
        "print(unique_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'servant', 'No', 'you', '?', 'together', 'forty-three', 'sovereign', \"'m\", 'maybe', 'That', 'ask', 'Will', 'if', 'winter', 'may', 'plover', 'defeator', 'our', 'five', 'Well', 'on', 'just', 'by', 'needs', 'its', 'pound', 'European', 'a', 'Oh', 'two', 'or', 'seek', 'use', 'strand', '!', 'warmer', 'interested', 'back', 'African', 'using', 'be', 'trusty', 'found', 'does', 'get', 'non-migratory', 'second', 'yeah', 'dorsal', 'Wait', 'I', 'go', 'agree', 'Ridden', 'with', 'wants', 'your', 'yet', 'is', 'Yes', \"'re\", 'martin', 'Halt', 'they', 'It', 'Please', 'England', 'them', 'land', 'Saxons', 'bring', 'all', 'could', 'weight', 'point', 'search', 'grips', 'Mercea', 'zone', 'King', 'Pendragon', 'question', \"'ve\", 'not', 'tropical', 'course', 'migrate', 'one', 'through', 'court', 'coconuts', 'The', 'Where', 'that', 'horse', 'empty', 'castle', 'an', 'beat', 'under', 'house', 'in', 'swallows', 'other', 'where', 'ARTHUR', 'Listen', 'speak', '[', \"'s\", '#', 'kingdom', 'Not', 'are', 'to', 'Arthur', 'strangers', 'carry', 'In', '2', 'suggesting', 'breadth', 'goes', 'every', '1', 'Pull', 'Camelot', \"'em\", 'bird', 'grip', ':', 'of', 'ratios', 'creeper', 'why', 'We', 'matter', 'line', '.', 'So', 'son', 'KING', 'since', 'but', 'fly', 'climes', 'my', 'ounce', 'maintain', 'from', 'coconut', 'Britons', 'south', 'sun', 'wings', 'A', 'there', 'it', 'order', 'minute', 'times', 'guiding', 'simple', 'air-speed', 'Uther', '...', 'What', 'carrying', 'these', 'swallow', 'knights', 'who', \"n't\", 'Am', 'halves', 'You', 'clop', 'ridden', 'master', 'Court', 'mean', 'Found', 'Patsy', 'lord', 'velocity', 'SOLDIER', 'SCENE', 'this', 'husk', 'me', 'Are', 'feathers', 'wind', 'join', 'got', 'Who', 'must', 'anyway', 'carried', 'will', 'right', 'then', 'am', '--', \"'\", 'tell', 'he', 'Supposing', ']', 'the', 'bangin', 'held', 'They', 'here', 'But', ',', 'covered', \"'d\", 'Whoa', 'do', 'length', 'at', 'temperate', 'have', 'snows', 'and'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAdole9fAFsv",
        "outputId": "8f3293f4-fcb0-4fb7-d9c8-329776506d79"
      },
      "source": [
        "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
        "match = re.search(\"coconuts\", scene_one)\n",
        "\n",
        "# Print the start and end indexes of match\n",
        "print(match.start(), match.end())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "580 588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uX_GxIx9D7Z2"
      },
      "source": [
        "# Import the necessary modules\n",
        "from nltk.tokenize import regexp_tokenize\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-o2cOfR9EMB_",
        "outputId": "05cefc0f-80ea-472a-91cd-c94c2aa0f340"
      },
      "source": [
        "tweets = ['This is the best #nlp exercise ive found online! #python',\n",
        " '#NLP is super fun! <3 #learning',\n",
        " 'Thanks @datacamp :) #nlp #python']\n",
        "\n",
        "pattern1 = r\"#\\w+\"\n",
        "# Use the pattern on the first tweet in the tweets list\n",
        "hashtags = regexp_tokenize(tweets[0], pattern1)\n",
        "print(hashtags)\n",
        "\n",
        "pattern2 = r\"([@#]\\w+)\"\n",
        "# Use the pattern on the last tweet in the tweets list\n",
        "mentions_hashtags = regexp_tokenize(tweets[-1], pattern2)\n",
        "print(mentions_hashtags)\n",
        "\n",
        "tknzr = TweetTokenizer()\n",
        "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
        "print(all_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#nlp', '#python']\n",
            "['@datacamp', '#nlp', '#python']\n",
            "[['This', 'is', 'the', 'best', '#nlp', 'exercise', 'ive', 'found', 'online', '!', '#python'], ['#NLP', 'is', 'super', 'fun', '!', '<3', '#learning'], ['Thanks', '@datacamp', ':)', '#nlp', '#python']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQKVf8vUEnz7",
        "outputId": "a922a992-8af3-41c8-98ab-ef3603df7b77"
      },
      "source": [
        "german_text = 'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
        "# Tokenize and print all words in german_text\n",
        "all_words = word_tokenize(german_text)\n",
        "print(all_words)\n",
        "\n",
        "# Tokenize and print only capital words\n",
        "capital_words = r\"[A-ZÜ]\\w+\"\n",
        "print(regexp_tokenize(german_text, capital_words))\n",
        "\n",
        "# Tokenize and print only emoji\n",
        "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
        "print(regexp_tokenize(german_text, emoji))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Wann', 'gehen', 'wir', 'Pizza', 'essen', '?', '🍕', 'Und', 'fährst', 'du', 'mit', 'Über', '?', '🚕']\n",
            "['Wann', 'Pizza', 'Und', 'Über']\n",
            "['🍕', '🚕']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "NiXe2JmGFiev",
        "outputId": "3ca8f402-1c92-467a-82e3-e0761d04e22a"
      },
      "source": [
        "#holy_grail = \"\"\n",
        "holy_grail = open('holy_grail.txt', 'r')\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Split the script into lines: lines\n",
        "lines = holy_grail.readlines()\n",
        "\n",
        "# Replace all script lines for speaker\n",
        "pattern = \"[A-Z]{2,}(\\s)?(#\\d)?([A-Z]{2,})?:\"\n",
        "lines = [re.sub(pattern, '', l) for l in lines]\n",
        "\n",
        "# Tokenize each line: tokenized_lines\n",
        "tokenized_lines = [regexp_tokenize(s, \"\\w+\") for s in lines]\n",
        "\n",
        "# Make a frequency list of lengths: line_num_words\n",
        "line_num_words = [len(t_line) for t_line in tokenized_lines]\n",
        "\n",
        "# Plot a histogram of the line lengths\n",
        "plt.hist(line_num_words)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "Counter(word_tokenize(\"Inserir Inserir qualquer texto nesse espaço e verificar o rsultado do texto\"))"
      ],
      "metadata": {
        "id": "Vnt0KYeBzYNP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b610e9d6-2191-49e7-bd27-e1af19df8a40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Inserir': 2,\n",
              "         'qualquer': 1,\n",
              "         'texto': 2,\n",
              "         'nesse': 1,\n",
              "         'espaço': 1,\n",
              "         'e': 1,\n",
              "         'verificar': 1,\n",
              "         'o': 1,\n",
              "         'rsultado': 1,\n",
              "         'do': 1})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKEl_gKXXpnE"
      },
      "source": [
        "Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QZnxbCsXni8",
        "outputId": "d6f87005-675a-4a99-d242-3af08d7b57f3"
      },
      "source": [
        "article = open('debugging.txt', 'r')\n",
        "\n",
        "# Import Counter\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the article: tokens\n",
        "tokens = word_tokenize(article.read())\n",
        "\n",
        "# Convert the tokens into lowercase: lower_tokens\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "# Create a Counter with the lowercase tokens: bow_simple\n",
        "bow_simple = Counter(lower_tokens)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow_simple.most_common(10))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(',', 151), ('the', 150), ('.', 89), ('of', 81), ('to', 63), ('a', 60), ('in', 44), (\"''\", 42), ('and', 41), ('(', 40)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMJh6fMhYxzh",
        "outputId": "03bc72aa-223e-4cda-9ec1-21af772ad201"
      },
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in stopwords.words('english')]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('debugging', 36), ('system', 25), ('software', 16), ('bug', 15), ('problem', 15), ('computer', 14), ('tool', 14), ('process', 13), ('term', 13), ('used', 12), ('http', 11), ('program', 11), ('debugger', 10), ('programming', 9), ('technique', 9), ('language', 9), ('code', 8), ('example', 8), ('check', 8), ('error', 7), ('also', 7), ('make', 7), ('programmer', 6), ('may', 6), ('acm', 6), ('would', 6), ('user', 6), ('case', 6), ('test', 6), ('ref', 6), ('see', 5), ('memory', 5), ('change', 5), ('debug', 5), ('article', 5), ('hardware', 5), ('task', 5), ('execution', 5), ('source', 5), ('wolf', 5), ('cite', 5), ('control', 4), ('testing', 4), ('dump', 4), ('design', 4), ('early', 4), ('proceeding', 4), ('use', 4), ('computing', 4), ('national', 4), ('anomaly', 4), ('impact', 4), ('word', 4), ('might', 4), ('determine', 4), ('often', 4), ('value', 4), ('variable', 4), ('original', 4), ('state', 4), ('tracing', 4), ('different', 4), ('fence', 4), ('algorithm', 4), ('embedded', 4), ('file', 3), ('mark', 3), ('moth', 3), ('grace', 3), ('hopper', 3), ('journal', 3), ('first', 3), ('meeting', 3), ('common', 3), ('various', 3), ('avoid', 3), ('made', 3), ('issue', 3), ('developer', 3), ('simple', 3), ('analysis', 3), ('breakpoints', 3), ('easier', 3), ('exception', 3), ('specific', 3), ('useful', 3), ('within', 3), ('detected', 3), ('reproduce', 3), ('environment', 3), ('crash', 3), ('line', 3), ('using', 3), ('part', 3), ('print', 3), ('statement', 3), ('command', 3), ('tron', 3), ('version', 3), ('remote', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('portuguese')"
      ],
      "metadata": {
        "id": "qL-VavBv3iP1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0df6ec7e-3b53-4ec4-8d48-2397bb6ccd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'à',\n",
              " 'ao',\n",
              " 'aos',\n",
              " 'aquela',\n",
              " 'aquelas',\n",
              " 'aquele',\n",
              " 'aqueles',\n",
              " 'aquilo',\n",
              " 'as',\n",
              " 'às',\n",
              " 'até',\n",
              " 'com',\n",
              " 'como',\n",
              " 'da',\n",
              " 'das',\n",
              " 'de',\n",
              " 'dela',\n",
              " 'delas',\n",
              " 'dele',\n",
              " 'deles',\n",
              " 'depois',\n",
              " 'do',\n",
              " 'dos',\n",
              " 'e',\n",
              " 'é',\n",
              " 'ela',\n",
              " 'elas',\n",
              " 'ele',\n",
              " 'eles',\n",
              " 'em',\n",
              " 'entre',\n",
              " 'era',\n",
              " 'eram',\n",
              " 'éramos',\n",
              " 'essa',\n",
              " 'essas',\n",
              " 'esse',\n",
              " 'esses',\n",
              " 'esta',\n",
              " 'está',\n",
              " 'estamos',\n",
              " 'estão',\n",
              " 'estar',\n",
              " 'estas',\n",
              " 'estava',\n",
              " 'estavam',\n",
              " 'estávamos',\n",
              " 'este',\n",
              " 'esteja',\n",
              " 'estejam',\n",
              " 'estejamos',\n",
              " 'estes',\n",
              " 'esteve',\n",
              " 'estive',\n",
              " 'estivemos',\n",
              " 'estiver',\n",
              " 'estivera',\n",
              " 'estiveram',\n",
              " 'estivéramos',\n",
              " 'estiverem',\n",
              " 'estivermos',\n",
              " 'estivesse',\n",
              " 'estivessem',\n",
              " 'estivéssemos',\n",
              " 'estou',\n",
              " 'eu',\n",
              " 'foi',\n",
              " 'fomos',\n",
              " 'for',\n",
              " 'fora',\n",
              " 'foram',\n",
              " 'fôramos',\n",
              " 'forem',\n",
              " 'formos',\n",
              " 'fosse',\n",
              " 'fossem',\n",
              " 'fôssemos',\n",
              " 'fui',\n",
              " 'há',\n",
              " 'haja',\n",
              " 'hajam',\n",
              " 'hajamos',\n",
              " 'hão',\n",
              " 'havemos',\n",
              " 'haver',\n",
              " 'hei',\n",
              " 'houve',\n",
              " 'houvemos',\n",
              " 'houver',\n",
              " 'houvera',\n",
              " 'houverá',\n",
              " 'houveram',\n",
              " 'houvéramos',\n",
              " 'houverão',\n",
              " 'houverei',\n",
              " 'houverem',\n",
              " 'houveremos',\n",
              " 'houveria',\n",
              " 'houveriam',\n",
              " 'houveríamos',\n",
              " 'houvermos',\n",
              " 'houvesse',\n",
              " 'houvessem',\n",
              " 'houvéssemos',\n",
              " 'isso',\n",
              " 'isto',\n",
              " 'já',\n",
              " 'lhe',\n",
              " 'lhes',\n",
              " 'mais',\n",
              " 'mas',\n",
              " 'me',\n",
              " 'mesmo',\n",
              " 'meu',\n",
              " 'meus',\n",
              " 'minha',\n",
              " 'minhas',\n",
              " 'muito',\n",
              " 'na',\n",
              " 'não',\n",
              " 'nas',\n",
              " 'nem',\n",
              " 'no',\n",
              " 'nos',\n",
              " 'nós',\n",
              " 'nossa',\n",
              " 'nossas',\n",
              " 'nosso',\n",
              " 'nossos',\n",
              " 'num',\n",
              " 'numa',\n",
              " 'o',\n",
              " 'os',\n",
              " 'ou',\n",
              " 'para',\n",
              " 'pela',\n",
              " 'pelas',\n",
              " 'pelo',\n",
              " 'pelos',\n",
              " 'por',\n",
              " 'qual',\n",
              " 'quando',\n",
              " 'que',\n",
              " 'quem',\n",
              " 'são',\n",
              " 'se',\n",
              " 'seja',\n",
              " 'sejam',\n",
              " 'sejamos',\n",
              " 'sem',\n",
              " 'ser',\n",
              " 'será',\n",
              " 'serão',\n",
              " 'serei',\n",
              " 'seremos',\n",
              " 'seria',\n",
              " 'seriam',\n",
              " 'seríamos',\n",
              " 'seu',\n",
              " 'seus',\n",
              " 'só',\n",
              " 'somos',\n",
              " 'sou',\n",
              " 'sua',\n",
              " 'suas',\n",
              " 'também',\n",
              " 'te',\n",
              " 'tem',\n",
              " 'tém',\n",
              " 'temos',\n",
              " 'tenha',\n",
              " 'tenham',\n",
              " 'tenhamos',\n",
              " 'tenho',\n",
              " 'terá',\n",
              " 'terão',\n",
              " 'terei',\n",
              " 'teremos',\n",
              " 'teria',\n",
              " 'teriam',\n",
              " 'teríamos',\n",
              " 'teu',\n",
              " 'teus',\n",
              " 'teve',\n",
              " 'tinha',\n",
              " 'tinham',\n",
              " 'tínhamos',\n",
              " 'tive',\n",
              " 'tivemos',\n",
              " 'tiver',\n",
              " 'tivera',\n",
              " 'tiveram',\n",
              " 'tivéramos',\n",
              " 'tiverem',\n",
              " 'tivermos',\n",
              " 'tivesse',\n",
              " 'tivessem',\n",
              " 'tivéssemos',\n",
              " 'tu',\n",
              " 'tua',\n",
              " 'tuas',\n",
              " 'um',\n",
              " 'uma',\n",
              " 'você',\n",
              " 'vocês',\n",
              " 'vos']"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "articles = open('articles.txt', 'r')\n",
        "articles = articles.readlines()\n",
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "tokenized_articles = [word_tokenize(doc.lower()) for doc in articles]\n",
        "# Create a Dictionary from the articles: dictionary\n",
        "dictionary = Dictionary(tokenized_articles)\n",
        "\n",
        "print(dictionary.token2id.get(\"transport\"))\n",
        "\n",
        "#dictionary.token2id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sS6wUfsuW8DY",
        "outputId": "a17586ce-20fa-4c49-fd05-9f65a274eb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "my_documents = ['The movie was about a spaceship and aliens.',\n",
        "                'I really liked the movie!',\n",
        "                'Awesome action scenes, but boring characters.',\n",
        "                'The movie was awful! I hate alien films.',\n",
        "                'Space is cool! I liked the movie.',\n",
        "                'More space films, please!',]\n",
        "\n",
        "tokenized_docs = [word_tokenize(doc.lower()) for doc in my_documents]\n",
        "dictionary = Dictionary(tokenized_docs)\n",
        "dictionary.token2id"
      ],
      "metadata": {
        "id": "MyYvrVH2a_Ic",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a23f8508-40f8-4c24-ed5c-e893efcff5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'.': 0,\n",
              " 'a': 1,\n",
              " 'about': 2,\n",
              " 'aliens': 3,\n",
              " 'and': 4,\n",
              " 'movie': 5,\n",
              " 'spaceship': 6,\n",
              " 'the': 7,\n",
              " 'was': 8,\n",
              " '!': 9,\n",
              " 'i': 10,\n",
              " 'liked': 11,\n",
              " 'really': 12,\n",
              " ',': 13,\n",
              " 'action': 14,\n",
              " 'awesome': 15,\n",
              " 'boring': 16,\n",
              " 'but': 17,\n",
              " 'characters': 18,\n",
              " 'scenes': 19,\n",
              " 'alien': 20,\n",
              " 'awful': 21,\n",
              " 'films': 22,\n",
              " 'hate': 23,\n",
              " 'cool': 24,\n",
              " 'is': 25,\n",
              " 'space': 26,\n",
              " 'more': 27,\n",
              " 'please': 28}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(doc) for doc in tokenized_docs]\n",
        "corpus"
      ],
      "metadata": {
        "id": "uidMzc8TrMtM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da293b2-f836-4d83-f59b-31b3ea492f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)],\n",
              " [(5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (12, 1)],\n",
              " [(0, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1)],\n",
              " [(0, 1),\n",
              "  (5, 1),\n",
              "  (7, 1),\n",
              "  (8, 1),\n",
              "  (9, 1),\n",
              "  (10, 1),\n",
              "  (20, 1),\n",
              "  (21, 1),\n",
              "  (22, 1),\n",
              "  (23, 1)],\n",
              " [(0, 1), (5, 1), (7, 1), (9, 1), (10, 1), (11, 1), (24, 1), (25, 1), (26, 1)],\n",
              " [(9, 1), (13, 1), (22, 1), (26, 1), (27, 1), (28, 1)]]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "tfidf = TfidfModel(corpus)\n",
        "tfidf[corpus[1]]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7N_TYwmrf1K",
        "outputId": "74f9899a-b300-4fc2-963b-ef341baffc6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(5, 0.1746298276735174),\n",
              " (7, 0.1746298276735174),\n",
              " (9, 0.1746298276735174),\n",
              " (10, 0.29853166221463673),\n",
              " (11, 0.47316148988815415),\n",
              " (12, 0.7716931521027908)]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFR40dSsbQ6J"
      },
      "source": [
        "Gensim - https://radimrehurek.com/gensim/"
      ]
    }
  ]
}